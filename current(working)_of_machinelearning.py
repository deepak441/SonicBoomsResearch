# -*- coding: utf-8 -*-
"""current(working) of MachineLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RXWYFV95KKMclMjsFNtWsbHpQIB4Ofs-
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, InputLayer

# Step 1: Generate synthetic non-linear data for sonic boom
def generate_nonlinear_sonic_boom_data(n_samples=1000):
    speed = np.random.uniform(1, 5, n_samples)
    altitude = np.random.uniform(0, 20, n_samples)
    angle_of_attack = np.random.uniform(0, 45, n_samples)

    amplitude = (
        10 * np.sin(speed) +
        0.5 * np.log(altitude + 1) -
        0.1 * np.cos(np.radians(angle_of_attack)) +
        np.random.normal(0, 1, n_samples)
    )
    duration = (
        0.1 * np.exp(speed / 2) +
        0.05 * np.sqrt(altitude) +
        0.01 * angle_of_attack**2 +
        np.random.normal(0, 0.1, n_samples)
    )

    X_raw = np.column_stack((speed, altitude, angle_of_attack))
    y = np.column_stack((amplitude, duration))
    return X_raw, y

# Step 2: Feature engineering
def generate_engineered_features(speed, altitude, angle):
   # features = np.column_stack([
   #     speed,
   #     altitude,
   #     angle,
   #     speed**2,
   #     np.sin(speed),
   #     np.sqrt(altitude),
   #     np.log(altitude + 1),
   #     angle**2,
   #     np.cos(np.radians(angle))
   # ])
    features = np.column_stack([speed,
                                 altitude,
                                 speed])
    return features

# Step 3: Load and preprocess data
X_raw, y = generate_nonlinear_sonic_boom_data()
speed, altitude, angle = X_raw[:, 0], X_raw[:, 1], X_raw[:, 2]
X = generate_engineered_features(speed, altitude, angle)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train)
y_test_scaled = scaler_y.transform(y_test)

# Reshape for deep learning models
X_train_cnn_rnn = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)
X_test_cnn_rnn = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)

# Step 4: Define and train classical models
models = [
    ('Linear Regression', LinearRegression()),
    ('Random Forest', RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42)),
    ('Support Vector Regressor', SVR()),
    ('Gradient Boosting', GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, random_state=42))
]

results_amplitude = []
results_duration = []
for name, model in models:
    model.fit(X_train_scaled, y_train_scaled[:, 0])
    y_pred_amp_scaled = model.predict(X_test_scaled).reshape(-1, 1)
    y_pred_amp_stacked = np.hstack((y_pred_amp_scaled, np.zeros_like(y_pred_amp_scaled)))
    y_pred_amp_unscaled = scaler_y.inverse_transform(y_pred_amp_stacked)[:, 0]
    mse_amp = mean_squared_error(y_test[:, 0], y_pred_amp_unscaled)
    print(f'{name} Mean Squared Error (Amplitude): {mse_amp:.4f}')
    results_amplitude.append((name, model, y_pred_amp_unscaled))

    model.fit(X_train_scaled, y_train_scaled[:, 1])
    y_pred_dur_scaled = model.predict(X_test_scaled).reshape(-1, 1)
    y_pred_dur_stacked = np.hstack((np.zeros_like(y_pred_dur_scaled), y_pred_dur_scaled))
    y_pred_dur_unscaled = scaler_y.inverse_transform(y_pred_dur_stacked)[:, 1]
    mse_dur = mean_squared_error(y_test[:, 1], y_pred_dur_unscaled)
    print(f'{name} Mean Squared Error (Duration): {mse_dur:.4f}')
    results_duration.append((name, model, y_pred_dur_unscaled))

# Step 5: Define CNN model
def build_cnn_model(input_shape):
    model = Sequential([
        InputLayer(input_shape=input_shape),
        Conv1D(32, kernel_size=2, activation='relu'),
        Flatten(),
        Dense(64, activation='relu'),
        Dense(2)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Step 6: Define RNN model
def build_rnn_model(input_shape):
    model = Sequential([
        LSTM(64, input_shape=input_shape),
        Dense(32, activation='relu'),
        Dense(2)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Step 7: Train and evaluate CNN and RNN
cnn_model = build_cnn_model(input_shape=(X_train_cnn_rnn.shape[1], 1))
cnn_model.fit(X_train_cnn_rnn, y_train_scaled, epochs=100, batch_size=32, validation_split=0.2, verbose=0)
y_pred_cnn_scaled = cnn_model.predict(X_test_cnn_rnn)
y_pred_cnn = scaler_y.inverse_transform(y_pred_cnn_scaled)
mse_cnn_amp = mean_squared_error(y_test[:, 0], y_pred_cnn[:, 0])
mse_cnn_dur = mean_squared_error(y_test[:, 1], y_pred_cnn[:, 1])
print(f'CNN Mean Squared Error (Amplitude): {mse_cnn_amp:.4f}')
print(f'CNN Mean Squared Error (Duration): {mse_cnn_dur:.4f}')
results_amplitude.append(('CNN', cnn_model, y_pred_cnn[:, 0]))
results_duration.append(('CNN', cnn_model, y_pred_cnn[:, 1]))

rnn_model = build_rnn_model(input_shape=(X_train_cnn_rnn.shape[1], 1))
rnn_model.fit(X_train_cnn_rnn, y_train_scaled, epochs=100, batch_size=32, validation_split=0.2, verbose=0)
y_pred_rnn_scaled = rnn_model.predict(X_test_cnn_rnn)
y_pred_rnn = scaler_y.inverse_transform(y_pred_rnn_scaled)
mse_rnn_amp = mean_squared_error(y_test[:, 0], y_pred_rnn[:, 0])
mse_rnn_dur = mean_squared_error(y_test[:, 1], y_pred_rnn[:, 1])
print(f'RNN Mean Squared Error (Amplitude): {mse_rnn_amp:.4f}')
print(f'RNN Mean Squared Error (Duration): {mse_rnn_dur:.4f}')
results_amplitude.append(('RNN', rnn_model, y_pred_rnn[:, 0]))
results_duration.append(('RNN', rnn_model, y_pred_rnn[:, 1]))

# Step 8: Plot predictions
def plot_predictions(results, y_test_true, label):
    plt.figure(figsize=(20, 4))
    min_val = min(y_test_true.min(), *(pred.min() for _, _, pred in results))
    max_val = max(y_test_true.max(), *(pred.max() for _, _, pred in results))
    for i, (name, model, y_pred) in enumerate(results):
        plt.subplot(1, len(results), i + 1)
        plt.scatter(y_test_true, y_pred, alpha=0.7, label='Predicted')
        plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Ideal')
        plt.xlabel(f'Actual {label}')
        plt.ylabel(f'Predicted {label}')
        plt.title(f'{name} Prediction')
        plt.legend()
    plt.tight_layout()
    plt.show()

plot_predictions(results_amplitude, y_test[:, 0], "Amplitude (dB)")
plot_predictions(results_duration, y_test[:, 1], "Duration (s)")

# Step 9: Plot residuals
def plot_residuals(results, y_test_true, label):
    plt.figure(figsize=(20, 4))
    for i, (name, model, y_pred) in enumerate(results):
        residuals = y_test_true - y_pred
        plt.subplot(1, len(results), i + 1)
        plt.scatter(y_test_true, residuals, alpha=0.6)
        plt.axhline(0, color='black', linestyle='--')
        plt.xlabel(f'Actual {label}')
        plt.ylabel('Residual')
        plt.title(f'{name} Residuals')
    plt.tight_layout()
    plt.show()

plot_residuals(results_amplitude, y_test[:, 0], "Amplitude (dB)")
plot_residuals(results_duration, y_test[:, 1], "Duration (s)")